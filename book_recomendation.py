# -*- coding: utf-8 -*-
"""Book_Recomendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-fkFUy4JK4XYXTZUTs70fsUHlR0VXiT

# Import Library
Import library yang dibutuhkan
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

"""# Data Loading
Download data dari kagle menggunakan API dan membacanya
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install -q kaggle

!kaggle datasets download -d arashnic/book-recommendation-dataset

!unzip -q book-recommendation-dataset.zip -d book_dataset

books = pd.read_csv('book_dataset/Books.csv', encoding='latin-1')
users = pd.read_csv('book_dataset/Users.csv', encoding='latin-1')
ratings = pd.read_csv('book_dataset/Ratings.csv', encoding='latin-1')

books.head()

"""Mengecek jumlah data"""

print('jumlah data buku: ', len(books.index))
print('jumlah data pengguna: ', len(users.index))
print('jumlah data rating: ', len(ratings.index))

"""# Univariate Exploratory Data Analysis

Tahap eksplorasi data penting untuk memahami karakteristik masing-masing variabel dalam dataset, serta membantu menentukan pendekatan pemodelan yang sesuai. Proses ini mencakup analisis terhadap tipe data, jumlah nilai unik, serta distribusi dari setiap variabel.

Dataset rekomendasi buku terdiri dari tiga variabel utama:
  - books : berisi informasi terkait buku seperti judul, penulis, dan tahun terbit.
  - users : berisi informasi profil pengguna seperti usia dan lokasi.
  - ratings : berisi data rating yang diberikan oleh pengguna terhadap buku

### Variabel Books
Variabel books berisi metadata dari buku-buku yang ada dalam sistem

#### cek struktur data
"""

books.info()

"""berdasarkan output diatas terdapat 271.360 entri dan 8 kolom

#### Rangkuman Statistik
"""

books.describe(include='all')

"""#### Cek masing masing kolom"""

print('Banyak data: ', books.index.stop)
print('Kolom pada data buku:\n', books.columns)

print(books['ISBN'].nunique())
print(books['ISBN'].value_counts())

"""ISBN harusnya unik. Berdasarkan output diatas tidak ada ISBN yang tidak unik atau duplikat."""

print(books['Book-Title'].nunique())
books['Book-Title'].value_counts().head(10).plot(kind='barh')

"""Melihat buku yang paling banyak muncul (top duplicate data)"""

print(books['Year-Of-Publication'].describe())
books['Year-Of-Publication'].astype(str).value_counts().sort_index().plot(kind='line')

"""terdapat outlier seperti tahun 0 atau > 2025 yang perlu dibersihkan"""

books['Publisher'].value_counts().head(10).plot(kind='barh')

"""Penerbit paling aktif

### Variabel Users
Variabel users berisi informasi pengguna

#### Cek struktur data
"""

users.info()

"""#### Rangkuman statistik"""

users.describe(include="all")

"""#### Cek masing masing kolom"""

print('Banyak data: ', users.index.stop)
print('Kolom pada data users:\n', users.columns)

print("Jumlah User-ID unik:", users['User-ID'].nunique())
print(users['User-ID'].value_counts())

users['Location'].value_counts().head(10).plot(kind='barh')

"""- Lokasi bisa diekstrak menjadi 3 bagian: kota, provinsi, dan negara
- Banyak data bisa berasal dari satu negara, dan bisa digunakan untuk analisis geografi
"""

print(users['Age'].describe())
users['Age'].hist(bins=50)

"""### Variabel Ratings

Variabel ratings adalah inti dari sistem rekomendasi akan di bangun. Data ini menunjukkan penilaian yang diberikan oleh pengguna terhadap suatu buku

#### Cek struktur data
"""

ratings.info()

"""#### Rangkuman statistik"""

ratings.describe(include="all")

print('Banyak data: ', ratings.index.stop)
print('Kolom pada data rating:\n', ratings.columns)

print("Jumlah user unik:", ratings['User-ID'].nunique())
ratings['User-ID'].value_counts().head(10).plot(kind='barh')

"""Melihat seberapa aktif user dalam memberikan rating"""

print("Jumlah ISBN unik:", ratings['ISBN'].nunique())
ratings['ISBN'].value_counts().head(10).plot(kind='barh')

"""buku mana yang paling banyak dinilai"""

print(ratings['Book-Rating'].describe())
ratings['Book-Rating'].value_counts().sort_index().plot(kind='bar')

"""- Nilai 0 berarti implicit rating (user melihat buku tapi tidak memberikan penilaian eksplisit).
- Nilai 1â€“10 adalah explicit rating.

# Data Preprocessing

Melakukan pembersihan dan penggabungan data dari beberapa sumber untuk memastikan data yang digunakan bersih, relevan, dan siap untuk proses modeling. Langkah-langkahnya mencakup penghapusan data kosong, duplikat, nilai tidak valid, serta penyatuan data ke dalam satu set yang konsisten

#### Preprocessing books Variable

cek nilai kosong (missing value)
"""

books.isnull().sum()

"""berdasarkan output diatas, terdapat nilai kosong di beberapa kolom variabel buku

membersihkan nilai kosong (missing value)
"""

books = books.dropna(subset=['Book-Author', 'Year-Of-Publication', 'Publisher', 'Image-URL-L'])
books.isnull().sum()

"""drop kolom image karena gambar tidak digunakan untuk modeling"""

books = books.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'])

"""cek judul buku yang duplikat"""

print("Jumlah judul buku yang duplikat:", books['Book-Title'].duplicated().sum())

duplicate_title_counts = books['Book-Title'].value_counts()
duplicate_titles = duplicate_title_counts[duplicate_title_counts > 1]

duplicate_books = books[books['Book-Title'].isin(duplicate_titles.index)][['Book-Title', 'ISBN']]

duplicate_books_unique = duplicate_books.drop_duplicates(subset='Book-Title')

duplicate_books_unique = duplicate_books_unique.merge(
    duplicate_titles.rename('Jumlah Duplikat'),
    left_on='Book-Title',
    right_index=True
)

duplicate_books_unique = duplicate_books_unique.sort_values(by='Jumlah Duplikat', ascending=False)
duplicate_books_unique.head(20)

"""membersihkan buku yang duplikat"""

books = books.drop_duplicates(subset='Book-Title', keep='first')
print("Jumlah judul buku yang duplikat:", books['Book-Title'].duplicated().sum())

"""Mengubah tipe data tahun publikasi ke int dan membersihkan outlier"""

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
books = books[(books['Year-Of-Publication'] > 1900) & (books['Year-Of-Publication'] <= 2025)]

books.info()

"""#### Preprocessing Users Variable

cek nilai kosong (missing value) data umur
"""

users['Age'].isnull().sum()

"""membersihkan nilai kosong dan outlier data umur"""

users['Age'] = users['Age'].fillna(0).astype(int)

users = users[(users['Age'] >= 13) & (users['Age'] <= 80)]

"""memisahkan fitur lokasi menjadi tiga bagian"""

users[['City', 'State', 'Country']] = users['Location'].str.split(',', expand=True, n=2)
users['Country'] = users['Country'].str.strip()
users = users.drop(columns=['Location'])

"""menghapus nilai kosong"""

users.dropna(subset=['City','State', 'Country'], inplace=True)

users.head()

"""#### Preprocessing Ratings Variable

mengecek nilai kosong
"""

ratings.isnull().sum()

"""menghapus semua baris yang punya Book-Rating = 0, karena itu dianggap tidak memberikan rating"""

ratings = ratings[ratings['Book-Rating'] > 0]

"""melihat distibusi rating"""

distribusi_rating = ratings['Book-Rating'].value_counts().sort_index()
distribusi_rating

"""menggaubngkan data menjadi satu"""

ratings_books = ratings.merge(books, on='ISBN', how='inner')
full_data = ratings_books.merge(users, on='User-ID', how='inner')

full_data.head()

"""membersihkan nilai n/a"""

mask = (full_data.astype(str)
         .apply(lambda col: col.str.strip().str.lower()) == 'n/a'
       ).any(axis=1)

full_data = full_data[~mask]

full_data.head()

"""# Data Preparation
mempersiapkan data untuk dua pendekatan sistem rekomendasi yaitu content-based dan collaborative filtering.

mengambil sampel data sebanyak 10k data
"""

full_data = full_data.sample(n=10000, random_state=42).reset_index(drop=True)

full_data.head(25)

"""Data preparation untuk content based filtering"""

full_data["all_features"] = full_data[["Book-Title", "Book-Author", "Publisher"]].astype(str).agg(" ".join, axis=1)

"""Data preparation untuk colaborative filtering"""

df_collab = full_data[['User-ID', 'ISBN', 'Book-Rating']].copy()
df_collab.rename(columns={
    'User-ID': 'userID',
    'ISBN': 'bookID',
    'Book-Rating': 'rating'
}, inplace=True)

df_collab.head()

"""Agar bisa digunakan dalam model, kita encode ID ke angka integer (Encode userID dan bookID)"""

user_ids = df_collab['userID'].unique().tolist()
user_to_encoded = {x: i for i, x in enumerate(user_ids)}
encoded_to_user = {i: x for i, x in enumerate(user_ids)}
df_collab['user'] = df_collab['userID'].map(user_to_encoded)

book_ids = df_collab['bookID'].unique().tolist()
book_to_encoded = {x: i for i, x in enumerate(book_ids)}
encoded_to_book = {i: x for i, x in enumerate(book_ids)}
df_collab['book'] = df_collab['bookID'].map(book_to_encoded)

"""gambaran umum dari data yang akan digunakan untuk pemodelan"""

num_users = df_collab['user'].nunique()
num_books = df_collab['book'].nunique()
df_collab['rating'] = df_collab['rating'].astype(np.float32)
min_rating = df_collab['rating'].min()
max_rating = df_collab['rating'].max()

print('Number of Users: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

"""#### Membagi Data untuk Training dan Validasi

Mengacak data agar distribusinya menjadi random
"""

df_collab = df_collab.sample(frac=1, random_state=42)

"""membuat variabel x dan y"""

x = df_collab[['user', 'book']].values
y = df_collab['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""Membagi data menjadi 80/20"""

train_size = int(0.8 * len(df_collab))
x_train, x_val = x[:train_size], x[train_size:]
y_train, y_val = y[:train_size], y[train_size:]
print(x, y)

"""# Content Based Filtering

Pada pendekatan content-based filtering, sistem merekomendasikan buku berdasarkan kemiripan atribut kontennya, seperti judul, penulis, dan penerbit. Dua teknik vektorisasi digunakan untuk membangun representasi teks:

- CountVectorizer: Mengubah fitur gabungan menjadi representasi frekuensi kata, lalu dihitung kemiripannya menggunakan cosine similarity.

- TfidfVectorizer: Menggunakan bobot TF-IDF untuk memberi nilai penting pada kata-kata yang lebih unik, lalu digunakan dalam perhitungan cosine similarity.

Fungsi rekomendasi akan mencari judul buku, menghitung kemiripan terhadap buku lain, dan mengembalikan daftar buku paling mirip sesuai jumlah rekomendasi yang diminta

#### Content Based Filtering Count Vectorizer

mengubah data teks buku jadi angka lalu menghitung kemiripan antar buku berdasarkan kontennya
"""

vectorizer_cv = CountVectorizer()
vectors_cv = vectorizer_cv.fit_transform(full_data["all_features"])

similarity_cv = cosine_similarity(vectors_cv)

"""rekomendasi buku yang mirip berdasarkan judul yang dimasukkan, menggunakan pendekatan Content-Based Filtering dengan CountVectorizer"""

def content_based_CountVectorizer(book_title, n_recommendations):
    book_title = str(book_title)

    if book_title not in full_data["Book-Title"].values:
        print("Judul tidak ditemukan dalam dataset.")
        return

    idx = full_data[full_data["Book-Title"] == book_title].index[0]

    sim_scores = list(enumerate(similarity_cv[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:n_recommendations+1]

    rekomendasi = []
    for i, score in sim_scores:
        title = full_data.iloc[i]["Book-Title"]
        author = full_data.iloc[i]["Book-Author"]
        publisher = full_data.iloc[i]["Publisher"]
        rekomendasi.append({
            "Book Title": title,
            "Author": author,
            "Publisher": publisher,
            "Similarity Score": round(score, 3)
        })

    result_df = pd.DataFrame(rekomendasi)
    return result_df

content_based_CountVectorizer("Stanislaski Sisters", n_recommendations=5)

"""#### Content Based Filtering TF-IDF Vectorizer

mengubah data teks buku menjadi vektor angka menggunakan TF-IDF lalu menghitung kemiripan antar buku
"""

vectorizer_tv = TfidfVectorizer()
vectors_tv = vectorizer_tv.fit_transform(full_data["all_features"])

similarity_tv = cosine_similarity(vectors_tv)

"""rekomendasi buku yang mirip dengan buku yang dicari, menggunakan metode Content-Based Filtering dengan TF-IDF"""

def content_based_TfidfVectorizer(book_title, n_recommendations=5):
    book_title = str(book_title)

    if book_title not in full_data["Book-Title"].values:
        print("Judul tidak ditemukan dalam dataset.")
        return None

    idx = full_data[full_data["Book-Title"] == book_title].index[0]

    sim_scores = list(enumerate(similarity_tv[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:n_recommendations+1]

    rekomendasi = []
    for i, score in sim_scores:
        title = full_data.iloc[i]["Book-Title"]
        author = full_data.iloc[i]["Book-Author"]
        publisher = full_data.iloc[i]["Publisher"]
        rekomendasi.append({
            "Book Title": title,
            "Author": author,
            "Publisher": publisher,
            "Similarity Score": round(score, 3)
        })

    result_df = pd.DataFrame(rekomendasi)
    return result_df

content_based_TfidfVectorizer("Stanislaski Sisters", n_recommendations=5)

"""# Collaborative Filtering

Pada pendekatan collaborative filtering, sistem menggunakan interaksi pengguna dan buku dalam bentuk rating. Model dibangun menggunakan jaringan neural sederhana dengan teknik embedding, yaitu merepresentasikan pengguna dan buku dalam vektor berdimensi rendah. Model mempelajari pola interaksi antar pengguna dan buku berdasarkan rating, dan diprediksi menggunakan fungsi aktivasi sigmoid. Data pelatihan dibagi ke dalam data training dan validasi dengan perbandingan 80:20. Model dilatih menggunakan binary crossentropy dan dievaluasi dengan metrik RMSE, dengan early stopping untuk menghindari overfitting

membuat model rekomendasi berbasis Collaborative Filtering dengan Neural Network menggunakan TensorFlow Keras
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.book_embedding = layers.Embedding(
            num_books, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])

        dot_user_book = tf.tensordot(user_vector, book_vector, 2)
        x = dot_user_book + user_bias + book_bias
        return tf.nn.sigmoid(x)

"""Menyiapkan model"""

model = RecommenderNet(num_users, num_books, embedding_size=50)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""membuat callback early stoping"""

earlystop_cb = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

"""training model"""

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=8,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=earlystop_cb
)

"""memvisualisasikan performa model selama proses pelatihan (training)"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model RMSE')
plt.ylabel('Root Mean Squared Error')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

"""merekomendasikan 10 buku terbaik untuk satu user acak berdasarkan prediksi rating dari model neural network collaborative filtering"""

user_id = df_collab['user'].sample(1).iloc[0]
books_visited_by_user = df_collab[df_collab['user'] == user_id]['book'].values

books_not_visited = df_collab[~df_collab['book'].isin(books_visited_by_user)]['book'].unique()
books_not_visited = np.array(list(set(books_not_visited)))

books_not_visited_encoded = [[book] for book in books_not_visited]
user_vector = np.array([[user_id]] * len(books_not_visited))
user_book_array = np.hstack((user_vector, books_not_visited_encoded))
ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_encoded_ids = [books_not_visited[x] for x in top_ratings_indices]

recommended_isbns = [encoded_to_book[int(encoded_id)] for encoded_id in recommended_encoded_ids]
recommended_titles = full_data[full_data['ISBN'].isin(recommended_isbns)][['ISBN', 'Book-Title']].drop_duplicates()

# Buku rating tinggi dari user
top_books_user_encoded = df_collab[df_collab['user'] == user_id]\
    .sort_values(by='rating', ascending=False)['book'].head(5).values
top_books_user_isbns = [encoded_to_book[int(book)] for book in top_books_user_encoded]
top_books_user_titles = full_data[full_data['ISBN'].isin(top_books_user_isbns)][['ISBN', 'Book-Title']].drop_duplicates()


print(f"Showing recommendations for user: {encoded_to_user[user_id]}")
print("=" * 30)

print("Books with high ratings from user")
print("-" * 40)
print(top_books_user_titles)

print("-" * 40)
print("Top 10 book recommendations")
print("-" * 40)
print(recommended_titles)